# Разбор кейса

Компания называется "Smart Home"

### Что сделано — документация

1. Текстовое описание старого решения с плюсами и минусами
2. Контекстная схема старого решения
3. Схема уровня компонент нового решения
4. Схема уровня контейнера нового решения
5. ERD нового решения
6. API документация микросервисов Телеметрии и Команд
7. Текстовое описание нового решения

Схемы: [ссылка](https://github.com/maxbolgarin/architecture-sprint-3/tree/sprint_3/docs)


### Что сделано — техническая часть

1. Реализованы два микросервиса на Go с использованием подхода 12 факторного приложения:
  * `telemetry` ([ссылка](https://github.com/maxbolgarin/architecture-sprint-3/tree/sprint_3/micro/telemetry)) для получения телеметрии из Kafka, сохранения ее в БД PostgreSQL и чтение оттуда
  * `commands` ([ссылка](https://github.com/maxbolgarin/architecture-sprint-3/tree/sprint_3/micro/commands)) для получения команд из Kafka от монолита, сохранения их в БД PostgreSQL и чтение оттуда для отчасти устройствам
2. Дополнен github CI для билда новых микросервисов и загрузки образов в registry
3. Написан helm chart ([ссылка](https://github.com/maxbolgarin/architecture-sprint-3/tree/sprint_3/charts/smart-home-monolith)) для развертывания монолиита, микросервисов телеметрии и команд, Kafka+zookeeper и postgresql.
4. Дополнен код монолита для отправки команд в Kafka, откуда они читаются микросервисом команд и записываются в PostgreSQL



### Запуск

1. Запуск кластера: `minikube start`
2. Запуск чарта: `helm upgrade --install smart-home ./charts/smart-home`
3. Создание топиков:
  * Commands: `kubectl exec -it $(kubectl get po -l app=kafka | tail -n 1 | awk '{print $1}') -- bin/kafka-topics.sh --create --topic commands --bootstrap-server localhost:9092`
  * Telemetry: `kubectl exec -it $(kubectl get po -l app=kafka | tail -n 1 | awk '{print $1}') -- bin/kafka-topics.sh --create --topic telemetry --bootstrap-server localhost:9092`
4. Включение ingress в minikube: `minikube addons enable ingress`
5. На MacOS для работы ingress - `sudo minikube tunnel`



## Текущее решение

### Описание

Smart Home — приложение, позволяющее пользователям управлять датчиками умного дома и отслеживать их состояние.

В данный момент приложение представляет из себя монолит, написанный на Java с применением фреймворка Spring. В качестве базы данных используется PostgreSQL.

Сборка выполняется с помощью Maven, приложение помещается в Docker-image. В GitHub настроен пайплайн, который билдит образ и отправляет в registry. Раскатка выполняется в Kubernetes, для этого написан helm chart и terraform.

### Функциональность

Монолит предоставляет 6 эндпоинтов для контроля сенсоров и получения информации:

* GET `/api/heating/{id}` — получение информации о системе отопления по ID;
* PUT `/api/heating/{id}` —  обновление информации о системе отопления по ID;
* POST `/api/heating/{id}/turn-on` — включение системы отопления по ID;
* POST `/api/heating/{id}/turn-off` — отключене системы отопления по ID;
* POST `/api/heating/{id}/set-temperature` - установка целевой температуры для системы отопления по ID;
* GET `/api/heating/{id}/current-temperature` - получение текущей температуры системы отопления по ID.

В базе данных есть две таблицы - `heating_systems`, которая используется в описанных выше эндпоинтах, и `temperature_sensors`, которая не используется в данный момент.

### Анализ функциональности

В системе в данный момент есть один домен — `heating_systems`, нагревательные системы. Датчики температур хоть и заявлены в описании, но по факту не реализованы.

Плюсы реализации: написан рабочий код для минимального продукта, несящего ценность. Код небольшой, всего один сервис, его легко понять и разворачивать.

Минусы реализации: не работает заявленный функционал, отсутствует возможность масштабирования на требование добавления датчиков с "будущее неуточненное поведение", так как текущая архитектура "хардкодит" тип датчика.


## Целевое решение

### Требования

Рассмотрим требования к целевой экосистеме, от которых будем отталкиваться при разработке:

> Экосистема доступна пользователю в режиме самообслуживания по модели SaaS.

Это означает, что есть web/мобильное приложение, в котором можно зарегистрироваться, добавлять свои устройства, управлять ими и отслеживать состояние.


> Система позволяет управлять отоплением, включать и выключать свет, запирать и отпирать автоматические ворота, удаленно наблюдать за домом и будущее неуточненное поведение.

У пользователя должна быть возможность добавлять устройства разных типов. Из этого сдедует, что приложение должно иметь generic подход к управлению устройствами, то есть не акцентироваться на конкретных реализациях.


> Пользователь самостоятельно выбирает необходимые ему модули умного дома (устройства), сам их подключает, настраивает сценарии работы и просматривает телеметрию.

Это должно быть реализовано через платформу, функциональность описана выше.


> Компания не занимается производством устройств, но поддерживает подключение к экосистеме устройств партнеров по стандартным протоколам.

Как говорилось выше, архитектура должна поддерживать добавление произвольных устройств, взаимодействующих по известному протоколу.


> Веб-разработка передана на аутсорс и не входит в требования данной работы.

Требуется реализация бэкенда, принимающего на себя взаимодействие с устройствами и отдающего информацию в удобном для себя формате, без акцентирования на предполагаемый формат выдачи информации на фронтенде.


> Модули управления приборами и сами приборы (устройства) должны быть максимально готовы к использованию и продаваться в отдельных комплектах для удобной покупки и подключения.

Описано выше, от системы требуется поддержка устройств, взаимодействующих по известному протоколу.


> Устройства должны быть доступны через интернет (для удаленного наблюдения и доступа), и предполагается, что пользователь будет иметь интернет-канал, к которому их можно подключить.

Информация об устройствах агрегируется на серверах системы, предоставляющей возможности взаимодействия с ними.


> Покупатели могут программировать систему для управления различными модулями в соответствии со своими потребностями.

Пользователи могут дополнять функциональность устройств согласно изветсному протоколу.


### Описание архитектуры

Опишем опыт пользователя (`users`). Он заходит на сайт, регистрируется. Попадает в ЛК, где ему предлагается создать дом (`houses`). Дом является "группой", в рамках которой существуют устройства. После создания дома пользователю предлагается добавить/подключить устройство (`devices`). Представим, что это датчик температуры компании Duck модели KEK123. Пользователь находит его в списке доступных устройств на сайте (`device_types`). Подключает его к приложению согласно описанной инструкции. Теперь устройство предоставляет метрики о своей работе (`telemetry`), а также им можно управлять, отправляя команды (`commands`). Если пользователь хочет подключить собранное своими руками в гараже устройство, которое измеряет поток нейтрино через его дом, он добавляет его описание, созданное на основе изестного протокола (`device_types`), а затем использует так же, как и обычное устройство.


### Микросервисы для бизнес задач

Согласно методологии DDD, будем создавать микросервис под каждую бизнес задачу:

* **Пользователи** — создание и обновление пользователей
* **Дома** — создание и обновление домов
* **Устройства** — добавление и обновление устройств
* **Типы устройств** — добавления описаний устройств
* **Управление устройствами** — принимает команды от пользователя и отдает устройствам
* **Телеметрия** — принятие, обработка, хранение и выдача результатов по телеметрии с устройств

Также будет использоваться текущий монолит, который можно переделать в микросервис для определенной задачи — ремаппинг "высокоуровневых" запросов вида "отключить систему отопления" в "вызвать команду 12 устройства ABC".


### Технические компоненты

На входе в систему будет расположен API Gateway, распределяющий внешние запросы по микросервисам. У каждого микросервиса будет своя база данных — PostgreSQL для всех (телеметрия лучше в ClickHouse, но для простоты в PostgreSQL).

Взаимодействие пользователя с системой происходит синхронно по REST API - добавление устройств, отправка команд и т.д. Сенсоры отправляют в бэкенд сообщения о телеметрии, которые сначала складываются в Kafka, откуда их читает микросервис телеметрии и пишет в БД. Также сенсоры делают запросы на получение новых команд, которые попадают в микросервис команд.


### Описание сущностей

* `User` — информация о пользователе
* `UserAuth` — авторизационная информация пользователя
* `House` — сущность "дом", группировка сенсоров, создаваемая пользователем
* `Device` - конкретный сенсор определенного типа, относится "много сенсоров к одному дому"
* `DeviceType` - описание сенсора определенного типа, хранит в себе айди метрик, которые присылаются сенсором, и айди команд, которые поддерживает сенсор
* `MetricType` - описание метрики, информации, которая отправляется сенсором в качестве телеметрии (сенсор отправляет произвольное количество разных метрик)
* `CommandType` - описания команд, которые могут быть применены на сенсоре
* `Telemetry` - значения метрик с сенсоров
* `CommandHistory` - история команд

Так как протокол заранее не установлен, то считаем, что сенсоры предоставляют телеметрию в виде метрик — аналога prometheus метрик. Каждый тип сенсора поставляет свой набор метрик, который описан в документации и перенесен в `DeviceType`.

Так как протокол заранее не установлен, то считаем, что сенсоры принимают команды в виде кода на bash или C, либо их код "захардкожен" и им нужно отправить факт срабатывания команды (`command_type == plain`).


### API

Практически для всех коммуникаций используется синхронный REST. Асинхронное взаимодействие через Kafka реализуется в двух случаях:
1. Прием телеметрии с устройств, она первым делом попадает в Kafka, а потом уже читается и обарабтывается микросервисом
2. Отправка команд устройствам, сначала они отправляются в Kafka, откуда вычитываются микросервисом и затем передаются устройствам


---


# Базовая настройка

## Запуск minikube

[Инструкция по установке](https://minikube.sigs.k8s.io/docs/start/)

```bash
minikube start
```


## Добавление токена авторизации GitHub

[Получение токена](https://github.com/settings/tokens/new)

```bash
kubectl create secret docker-registry ghcr --docker-server=https://ghcr.io --docker-username=<github_username> --docker-password=<github_token> -n default
```


## Установка API GW kusk

[Install Kusk CLI](https://docs.kusk.io/getting-started/install-kusk-cli)

```bash
kusk cluster install
```


## Настройка terraform

[Установите Terraform](https://yandex.cloud/ru/docs/tutorials/infrastructure-management/terraform-quickstart#install-terraform)


Создайте файл ~/.terraformrc

```hcl
provider_installation {
  network_mirror {
    url = "https://terraform-mirror.yandexcloud.net/"
    include = ["registry.terraform.io/*/*"]
  }
  direct {
    exclude = ["registry.terraform.io/*/*"]
  }
}
```

## Применяем terraform конфигурацию 

```bash
cd terraform
terraform apply
```

## Настройка API GW

```bash
kusk deploy -i api.yaml
```

## Проверяем работоспособность

```bash
kubectl port-forward svc/kusk-gateway-envoy-fleet -n kusk-system 8080:80
curl localhost:8080/hello
```


## Delete minikube

```bash
minikube delete
```
